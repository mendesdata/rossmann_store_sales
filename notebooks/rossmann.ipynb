{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 General Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost           as xgb\n",
    "\n",
    "import math \n",
    "import inflection\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from sklearn.metrics       import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "from IPython.display       import Image\n",
    "from tabulate              import tabulate\n",
    "from boruta                import BorutaPy\n",
    "\n",
    "from scipy                 import stats as ss\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns( df ):\n",
    "    title = lambda x: inflection.titleize( x )\n",
    "    snakecase = lambda x: inflection.underscore( x )\n",
    "    spaces = lambda x: x.replace(\" \", \"\")\n",
    "\n",
    "    cols_old = list( df.columns )\n",
    "    cols_old = list( map( title, cols_old ) )\n",
    "    cols_old = list( map( spaces, cols_old ) )\n",
    "    cols_new = list( map( snakecase, cols_old ) )\n",
    "    \n",
    "    df.columns = cols_new\n",
    "\n",
    "    return df\n",
    "\n",
    "def fillout_na( df ):\n",
    "    # competition_distance with NA values means \"no competitor around\". Set max value = 200000\n",
    "    max_value = df['competition_distance'].max()\n",
    "    df['competition_distance'] = df['competition_distance'].apply( lambda x: 200000 if math.isnan( x ) else x )\n",
    "\n",
    "    # competition_open_since_month and competition_open_since_year \n",
    "    # Set month and year of sale as default value\n",
    "    df['competition_open_since_month'] = df.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
    "    df['competition_open_since_year']  = df.apply( lambda x: x['date'].year if math.isnan(  x['competition_open_since_year'] )  else x['competition_open_since_year'], axis=1 )\n",
    "\n",
    "    # promo2_since_week  and promo2_since_year\n",
    "    # Set month and year of sale as default value\n",
    "    df['promo2_since_week'] = df.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
    "    df['promo2_since_year'] = df.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
    "\n",
    "    # promo_interval  - first, create a new column(month_map) with the month of sale  \n",
    "    month_map = { 1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec' }\n",
    "    df['promo_interval'].fillna( 0, inplace=True )\n",
    "    df['month_map'] = df['date'].dt.month.map( month_map )\n",
    "\n",
    "    # promo_inteval - second, create a new column(is_promo) to check two conditions:  if promo_interval is active (1) and if month_map is inside promo_interval. (0) No, (1) Yes\n",
    "    df['is_promo'] = df[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 ) \n",
    "\n",
    "    return df\n",
    "\n",
    "def change_data_types( df ):\n",
    "    df['competition_open_since_month']  = df['competition_open_since_month'].astype( int )\n",
    "    df['competition_open_since_year']   = df['competition_open_since_year'].astype( int )\n",
    "    df['promo2_since_week']             = df['promo2_since_week'].astype( int )\n",
    "    df['promo2_since_year']             = df['promo2_since_year'].astype( int )\n",
    "\n",
    "    return df \n",
    "\n",
    "\n",
    "def data_cleaning( df ):\n",
    "\n",
    "    df = rename_columns( df )\n",
    "    df = fillout_na( df )\n",
    "    df = change_data_types( df )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_features( df ):\n",
    "    # year\n",
    "    df['year'] = df['date'].dt.year\n",
    "\n",
    "    # month\n",
    "    df['month'] = df['date'].dt.month\n",
    "\n",
    "    # day\n",
    "    df['day'] = df['date'].dt.day\n",
    "\n",
    "    # week of year\n",
    "    df['week_of_year'] = df.apply( lambda x: datetime.date( x['year'], x['month'], x['day'] ).isocalendar().week, axis=1 )\n",
    "\n",
    "    # year week\n",
    "    df['year_week'] = df.apply( lambda x: str( x['year'] ) + '-' + str( x['week_of_year'] ), axis=1 )\n",
    "\n",
    "    # from competition_open_since columns\n",
    "    df['competition_since']      = df.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'], day=1 ), axis=1 )\n",
    "    df['competition_time_month'] = df.apply( lambda x: ( ( x['date'] - x['competition_since'] ) / 30 ).days, axis=1 ).astype( int )\n",
    "\n",
    "    # from promo2_since\n",
    "    df['promo2_since']     = df['promo2_since_year'].astype( str ) + '-' + df['promo2_since_week'].astype( str )\n",
    "    df['promo2_since']     = df['promo2_since'].apply( lambda x: datetime.datetime.strptime( x +'-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "    df['promo2_time_week'] = df.apply( lambda x: ( ( x['date'] - x['promo2_since'] ) / 7 ).days, axis=1 ).astype( int )\n",
    "\n",
    "    #  describes an assortment level: a = basic, b = extra, c = extended\n",
    "    df['assortment'] = df['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x =='b' else 'extended' )\n",
    "\n",
    "    # Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. \n",
    "    # a = public holiday, b = easter holiday, c = christmas, 0 = regular day\n",
    "    df['state_holiday'] = df['state_holiday'].apply( lambda x: 'public holiday' if x == 'a' else 'easter holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular day' )\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_rows( df ):\n",
    "    # remove lines where there are no sales\n",
    "    df = df[ ( df['open'] != 0 ) & ( df['sales'] > 0 ) ]\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_cols( df ):\n",
    "    # customers -> Quantidade de clientes nas lojas, não é possível saber quantos clientes estarão nas lojas na predição. \n",
    "    # open -> Se a loja está aberta ou não. Quando está fechada não há vendas, ou seja, devem ser consideradas apenas as linhas com loja aberta ( open != 0 )\n",
    "    # sales -> Valor total em vendas. Quando não há vendas desconsiderar linhas ( sales > 0 )\n",
    "    # colunas que foram criadas apenas para auxiliar a geração de outras também devem ser excluídas. Ex: promo_interval, month_map\n",
    "\n",
    "    cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "    df = df.drop( cols_drop, axis=1 )\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_engineering( df ):\n",
    "    df = create_features( df )\n",
    "    df = remove_rows( df )\n",
    "    df = drop_cols( df )\n",
    "\n",
    "    return df\n",
    "\n",
    "def rescaling_features( df, df_type ):\n",
    "    # Apply MinMaxScaler - promo2_time_week\n",
    "    mms = MinMaxScaler()\n",
    "    df['promo2_time_week'] = mms.fit_transform( df[['promo2_time_week']].values )\n",
    "    if df_type == 'train':\n",
    "        pickle.dump( mms, open( '../parameters/promo2_time_week_scaler.pkl', 'wb' ) )\n",
    "\n",
    "    # Apply MinMaxScaler - year\n",
    "    mms = MinMaxScaler()\n",
    "    df['year'] = mms.fit_transform( df[['year']].values )\n",
    "    if df_type == 'train':\n",
    "        pickle.dump( mms, open( '../parameters/year_scaler.pkl', 'wb' ) )\n",
    "\n",
    "    # Apply RobustScaler - competition_distance\n",
    "    rs = RobustScaler()\n",
    "    df['competition_distance'] = rs.fit_transform( df[['competition_distance']].values )\n",
    "    if df_type == 'train':    \n",
    "        pickle.dump( rs, open( '../parameters/competition_distance_scaler.pkl', 'wb' ) )\n",
    "\n",
    "    # Apply RobustScaler - competition_time_month\n",
    "    rs = RobustScaler()\n",
    "    df['competition_time_month'] = rs.fit_transform( df[['competition_time_month']].values )\n",
    "    if df_type == 'train':\n",
    "        pickle.dump( rs, open( '../parameters/competition_time_month_scaler.pkl', 'wb' ) )\n",
    "\n",
    "    return df \n",
    "\n",
    "def encoding_features( df, df_type ):\n",
    "    # Apply One-Hot Encoding - state_holiday\n",
    "    df = pd.get_dummies( df, prefix=['state_holiday'], columns=['state_holiday'] )     \n",
    "\n",
    "    # Apply Label Encoding - store_type\n",
    "    le = LabelEncoder()\n",
    "    df['store_type'] = le.fit_transform( df['store_type'])\n",
    "    if df_type == 'train':\n",
    "        pickle.dump( le, open( '../parameters/store_type_scaler.pkl', 'wb' ) )       \n",
    "\n",
    "    # Apply Ordinal Encoding - assortment\n",
    "    assortment_dict = { 'basic' : 1, 'extra' : 2, 'extended' : 3 }\n",
    "    df['assortment'] = df['assortment'].map( assortment_dict )\n",
    "\n",
    "    return df\n",
    "\n",
    "def magnitude_features( df ):\n",
    "    # Apply Logarithm Transformation - response variable (sales)\n",
    "    df['sales'] = np.log1p( df['sales'] ) \n",
    "\n",
    "    return df\n",
    "\n",
    "def ciclyc_features( df ):\n",
    "    # Calculate sin and cos - month - variable with cyclic behavior\n",
    "    df['month_sin'] = df['month'].apply( lambda x: np.sin( x * ( 2 * np.pi/12 ) ) )\n",
    "    df['month_cos'] = df['month'].apply( lambda x: np.cos( x * ( 2 * np.pi/12 ) ) )\n",
    "\n",
    "    # Calculate sin and cos - day_of_week - variable with cyclic behavior\n",
    "    df['day_of_week_sin'] = df['day_of_week'].apply( lambda x: np.sin( x * ( 2 * np.pi/7 ) ) )\n",
    "    df['day_of_week_cos'] = df['day_of_week'].apply( lambda x: np.cos( x * ( 2 * np.pi/7 ) ) )\n",
    "\n",
    "    # Calculate sin and cos - day - variable with cyclic behavior\n",
    "    df['day_sin'] = df['day'].apply( lambda x: np.sin( x * ( 2 * np.pi/30 ) ) )\n",
    "    df['day_cos'] = df['day'].apply( lambda x: np.cos( x * ( 2 * np.pi/30 ) ) )\n",
    "\n",
    "    # Calculate sin and cos - week_of_year - variable with cyclic behavior\n",
    "    df['week_of_year_sin'] = df['week_of_year'].apply( lambda x: np.sin( x * ( 2 * np.pi/52 ) ) )\n",
    "    df['week_of_year_cos'] = df['week_of_year'].apply( lambda x: np.cos( x * ( 2 * np.pi/52 ) ) )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_preparation( df, df_type ):\n",
    "\n",
    "    # apply rescaling methods \n",
    "    df = rescaling_features( df, df_type )   \n",
    "\n",
    "    # apply enconding methods to categorical features\n",
    "    df = encoding_features( df, df_type )\n",
    "\n",
    "    # change features magnitude\n",
    "    df = magnitude_features( df )\n",
    "\n",
    "    # apply method to features with cyclical behavior\n",
    "    df = ciclyc_features( df )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# returns cramer´s v\n",
    "def cramer_v( x, y ):\n",
    "    # create confunsion matrix \n",
    "    cm = pd.crosstab( x, y ).values\n",
    "\n",
    "    # sum of confusion matrix\n",
    "    n = cm.sum()\n",
    "\n",
    "    # confusion matrix - rows and cols\n",
    "    r, k = cm.shape\n",
    "\n",
    "    # calculate chi2\n",
    "    chi2 = ss.chi2_contingency( cm )[0]    \n",
    "\n",
    "    # calculate adjusted chi2, k, r\n",
    "    chi2_adj = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
    "    k_adj = k - (k-1)**2/(n-1)\n",
    "    r_adj = r - (r-1)**2/(n-1)\n",
    "\n",
    "    # calculate and return cramer´s v\n",
    "    return np.sqrt( ( chi2_adj/n ) / ( min( k_adj-1, r_adj-1) ) )    \n",
    "\n",
    "\n",
    "def mean_percentage_error( y, yhat ):\n",
    "    return np.mean( ( y - yhat ) / y )\n",
    "\n",
    "#def mean_absolute_percentage_error( y, yhat ):\n",
    "#    return np.round( np.mean( np.abs( ( y - yhat ) / y ) ) * 100, 2 )\n",
    "\n",
    "def ml_error( model_name, y, yhat ):\n",
    "    mae  = mean_absolute_error( y, yhat )\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
    "\n",
    "    return pd.DataFrame( { 'Model Name' : model_name,\n",
    "                           'MAE'        : mae,\n",
    "                           'MAPE'       : mape,\n",
    "                           'RMSE'       : rmse\n",
    "                           }, index=[0] )\n",
    "\n",
    "def cross_validation( x_training, kfold, model_name, model, verbose=False ):\n",
    "    mae_list =  []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    for k in reversed( range( 1, kfold+1 ) ):\n",
    "        # start and end date for validation\n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta( days=k*6*7 )\n",
    "        validation_end_date   = x_training['date'].max() - datetime.timedelta( days=( k-1 )*6*7 )\n",
    "\n",
    "        if verbose:\n",
    "            print(f'\\nValidation Block: { k }. Dates: { validation_start_date } - { validation_end_date }' )\n",
    "\n",
    "                                                                            \n",
    "        # filtering dataset\n",
    "        training   = x_training[ x_training['date'] <= validation_start_date]\n",
    "        validation = x_training[ (x_training['date'] > validation_start_date) & (x_training['date'] <= validation_end_date) ]\n",
    "\n",
    "        # training dataset\n",
    "        xtraining = training.drop( ['date', 'sales'], axis=1 )\n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation\n",
    "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit( xtraining, ytraining )\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict( xvalidation )\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n",
    "\n",
    "        # store performance of each kfold iteration\n",
    "        mae_list.append( m_result['MAE'] )\n",
    "        mape_list.append( m_result['MAPE'] )\n",
    "        rmse_list.append( m_result['RMSE'] )\n",
    "\n",
    "    return pd.DataFrame( {  'Model Name': model_name,\n",
    "                            'MAE CV'  : np.round( np.mean( mae_list ),  2 ).astype( str ) + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n",
    "                            'MAPE CV' : np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n",
    "                            'RMSE CV' : np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype( str )\n",
    "                        }, index=[0] )\n",
    "\n",
    "\n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    #%pylab inline\n",
    "\n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "\n",
    "    #display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "\n",
    "    sns.set()\n",
    "\n",
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "df_train_raw = pd.read_csv ( '../data/train.csv', low_memory= False )\n",
    "df_store_raw = pd.read_csv ( '../data/store.csv', low_memory= False )\n",
    "\n",
    "# merging datasets train and store\n",
    "df_train_raw = pd.merge( df_train_raw, df_store_raw, how='left', on='Store')\n",
    "print(f'Dataset Dimension: rows: { df_train_raw.shape[0] }  cols:  { df_train_raw.shape[1] } ' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Split Raw Train Dataframe in two: training and test( last 6 weeks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data type - date column\n",
    "df_train_raw['Date'] = pd.to_datetime( df_train_raw['Date'] )\n",
    "\n",
    "# calculate period - last 6  weeks\n",
    "last_date  = df_train_raw[['Store', 'Date']].groupby( 'Store' ).max().reset_index().loc[0, 'Date']\n",
    "first_date = last_date - datetime.timedelta( days=6*7 )\n",
    "\n",
    "# creating train and test datasets\n",
    "df_train = df_train_raw.copy() \n",
    "df_test  = df_train_raw.copy() \n",
    "\n",
    "df_train = df_train[ df_train['Date'] <= first_date ]\n",
    "df_test  = df_test[  df_test['Date'] > first_date ]\n",
    "\n",
    "full_dataset_rows  = df_train_raw.shape[0]\n",
    "train_dataset_rows = df_train.shape[0]\n",
    "test_dataset_rows  = df_test.shape[0]\n",
    "\n",
    "print(f'First date Train: { df_train[\"Date\"].min() } ')\n",
    "print(f'Last date Train : { df_train[\"Date\"].max() } ')\n",
    "\n",
    "print(f'\\nFirst date Test: { df_test[\"Date\"].min() } ')\n",
    "print(f'Last date Test :   { df_test[\"Date\"].max() } ')\n",
    "\n",
    "print(f'\\nFull Dataset rows: { full_dataset_rows } ' )\n",
    "print(f'Train Dataset rows: { train_dataset_rows } { round( train_dataset_rows / full_dataset_rows * 100, 2) } %' )\n",
    "print(f'Test Dataset rows: { test_dataset_rows }   { round( test_dataset_rows / full_dataset_rows * 100,  2) } %' )\n",
    "\n",
    "df_test = rename_columns( df_test )\n",
    "df_test.to_csv( '../data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = rename_columns( df1 )\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Rows: { df1.shape[0] }' )\n",
    "print(f'Number of Cols: { df1.shape[1] }' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 List Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list data types\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows with NA values by column\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Fillout NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = fillout_na( df1 )\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Change Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = change_data_types( df1 )\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Descriptive Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and separate numerical and categorical attrributes\n",
    "df_num_attributes = df1.select_dtypes( include=['int64', 'float64'] )\n",
    "df_cat_attributes = df1.select_dtypes( exclude=['int64', 'float64', 'datetime64[ns]'] )\n",
    "\n",
    "df_cat_attributes.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Tendency - mean, median\n",
    "df_mean = pd.DataFrame( df_num_attributes.apply( np.mean ) ).T\n",
    "df_median = pd.DataFrame( df_num_attributes.apply( np.median ) ).T\n",
    "\n",
    "# Dispersion - min, max, range, std, skew, kurtosis\n",
    "df_min =      pd.DataFrame( df_num_attributes.apply( np.min ) ).T\n",
    "df_max =      pd.DataFrame( df_num_attributes.apply( np.max ) ).T\n",
    "df_range =    pd.DataFrame( df_num_attributes.apply( lambda x: x.max() - x.min() ) ).T\n",
    "df_std =      pd.DataFrame( df_num_attributes.apply( np.std ) ).T\n",
    "df_skew =     pd.DataFrame( df_num_attributes.apply( lambda x: x.skew() ) ).T\n",
    "df_kurtosis = pd.DataFrame( df_num_attributes.apply( lambda x: x.kurtosis() ) ).T\n",
    "\n",
    "# concatenate dataframes\n",
    "df_numerical = pd.concat( [df_min, df_max, df_range, df_mean, df_median, df_std, df_skew, df_kurtosis] ).T.reset_index()\n",
    "df_numerical.columns =  ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis'] \n",
    "df_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_attributes.apply( lambda x: x.unique().shape[0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_aux = df1[ ( df1['state_holiday'] != 0 )  & ( df1['sales'] > 0 ) ]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.suptitle('Categorial Attributes')\n",
    "\n",
    "sns.boxplot( ax=axes[0], x='state_holiday', y='sales', data=df1_aux );\n",
    "sns.boxplot( ax=axes[1], x='store_type',    y='sales', data=df1_aux );\n",
    "sns.boxplot( ax=axes[2], x='assortment',    y='sales', data=df1_aux );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hypothesis Mind Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image( '../img/mapa_de_hipoteses.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 List of Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Store Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas com maior número de funcionários deveriam vender mais \n",
    "\n",
    "**2.** Lojas com maior estoque deveriam vender mais \n",
    "\n",
    "**3.** Lojas de maior porte deveriam vender mais \n",
    "\n",
    "**4.** Lojas com maior sortimento deveriam vender mais \n",
    "\n",
    "**5.** Lojas com competidores mais próximos deveriam vender menos\n",
    "\n",
    "**6.** Lojas com competidores a mais tempo deveriam vender mais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Product Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas com maior investimento em marketing deveriam vender mais\n",
    "\n",
    "**2.** Lojas que expõem mais produtos na vitrine deveriam vender mais\n",
    "\n",
    "**3.** Lojas com produtos com preço menores deveriam vender mais\n",
    "\n",
    "**4.** Lojas com produtos com preço menores por mais tempo deveriam vender mais\n",
    "\n",
    "**5.** Lojas com promoções mais agressivas deveriam vender mais\n",
    "\n",
    "**6.** Lojas com promoções ativas a mais tempo deveriam vender mais\n",
    "\n",
    "**7.** Lojas com mais dias de promoção deveriam vender mais\n",
    "\n",
    "**8.** Lojas com mais promoções consecutivas deveriam vender mais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Time Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas abertas durante o feriado de Natal deveriam vender mais\n",
    "\n",
    "**2.** Lojas deveriam vender mais ao longo dos anos\n",
    "\n",
    "**3.** Lojas deveriam vender mais no segundo semestre do ano\n",
    "\n",
    "**4.** Lojas deveriam vender mais depois do dia 10 de cada mês\n",
    "\n",
    "**5.** Lojas deveriam vender menos nos finais de semana\n",
    "\n",
    "**6.** Lojas deveriam vender menos durante os feriados escolares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Priority List of Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas com maior sortimento deveriam vender mais (STORE)\n",
    "\n",
    "**2.** Lojas com competidores mais próximos deveriam vender menos (STORE)\n",
    "\n",
    "**3.** Lojas com competidores a mais tempo deveriam vender mais (STORE)\n",
    "\n",
    "\n",
    "\n",
    "**4.** Lojas com promoções ativas a mais tempo deveriam vender mais (PRODUCT)\n",
    "\n",
    "**5.** Lojas com mais dias de promoção deveriam vender mais (PRODUCT)\n",
    "\n",
    "**6.** Lojas com mais promoções consecutivas deveriam vender mais (PRODUCT)\n",
    "\n",
    "\n",
    "\n",
    "**7.** Lojas abertas durante o feriado de Natal deveriam vender mais (TIME)\n",
    "\n",
    "**8.** Lojas deveriam vender mais ao longo dos anos (TIME)\n",
    "\n",
    "**9.** Lojas deveriam vender mais no segundo semestre do ano (TIME)\n",
    "\n",
    "**10.** Lojas deveriam vender mais depois do dia 10 de cada mês (TIME)\n",
    "\n",
    "**11.** Lojas deveriam vender menos nos finais de semana (TIME)\n",
    "\n",
    "**12.** Lojas deveriam vender menos durante os feriados escolares (TIME)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = create_features( df2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 From date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['date', 'year', 'month', 'day', 'week_of_year', 'year_week']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 From competition_open_since columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['date', 'competition_open_since_year', 'competition_open_since_month', 'competition_since', 'competition_time_month']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 From promo2_since columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['promo2_since_year', 'promo2_since_week', 'promo2_since', 'date', 'promo2_time_week']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 From assorment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['assortment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 From state_holiday column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['state_holiday'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Filtering Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Filtering Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = remove_rows( df2 )\n",
    "df2[ ( df2['sales'] == 0 ) | ( df2['open'] == 0 ) ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Filtering Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = drop_cols( df2 )\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking distribuition \n",
    "sns.displot( df3['sales'], kde='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheking distribuition\n",
    "df_num_attributes.hist( bins=25, figsize=(20, 10) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notas do gráfico: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** A variável \"competition_open_since_month\" apresenta vários comportamentos, e quantos mais comportamentos uma variável possui melhor para o aprendizado do modelo.\n",
    "\n",
    "**2.** Já a variável \"day_of_week\" apresenta um comportamente constante, não há variação, esse tipo de variável sozinha não serve para o aprendizado modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "fig.suptitle('Categorial variables - Counter and Sales Density')\n",
    "\n",
    "# state_holiday\n",
    "sns.countplot( ax=axes[0,0], data=df3[ df3['state_holiday'] !=  'regular day' ]['state_holiday'] );\n",
    "sns.kdeplot  ( ax=axes[0,1], data=df3[ df3['state_holiday'] !=  'regular day' ][['state_holiday', 'sales']], x='sales',  hue='state_holiday', fill=True );\n",
    "\n",
    "# store_type\n",
    "sns.countplot( ax=axes[1,0], data=df3['store_type'] );\n",
    "sns.kdeplot  ( ax=axes[1,1], data=df3[['store_type', 'sales']], x='sales',  hue='store_type', fill=True );\n",
    "\n",
    "# assortment\n",
    "sns.countplot( ax=axes[2,0], data=df3['assortment'] );\n",
    "sns.kdeplot  ( ax=axes[2,1], data=df3[['assortment', 'sales']], x='sales',  hue='assortment', fill=True );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1 - Lojas com maior sortimento deveriam vender mais (STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***FALSA*** Lojas com MAIOR SORTIMENTO vendem MENOS\n",
    "\n",
    "***ASSORTMENT*** Apresenta comportamento variável na classe \"extra\", mas com um volume pequeno de dados, portanto, relevância BAIXA para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales x assortments - total\n",
    "aux = df3[['assortment', 'sales']].groupby( 'assortment' ).sum().reset_index()\n",
    "sns.barplot( x='assortment', y='sales', data=aux );\n",
    "\n",
    "# sales x assortments - timeline \n",
    "aux2 = df3[['year_week', 'assortment', 'sales']].groupby( ['year_week', 'assortment'] ).sum().reset_index()\n",
    "aux2.pivot( index='year_week', columns='assortment', values='sales' ).plot();\n",
    "\n",
    "# sales x assortments - only \"extra\" assortment\n",
    "aux3 = aux2[ aux2['assortment'] == 'extra']\n",
    "aux3.pivot( index='year_week', columns='assortment', values='sales' ).plot();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2 - Lojas com competidores mais próximos deveriam vender menos (STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FALSA** Lojas com COMPETIDORES MAIS PRÓXIMOS vendem MAIS\n",
    "\n",
    "**COMPETITION_DISTANCE** apresenta uma correlação FRACA com a variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10) )\n",
    "fig.suptitle('Competition Distance Behavior')\n",
    "\n",
    "\n",
    "# sales x competition_distance - total\n",
    "aux1 = df3[['competition_distance', 'sales']].groupby( 'competition_distance' ).sum().reset_index()\n",
    "sns.scatterplot( ax=axes[0], x='competition_distance', y='sales', data=aux1 );\n",
    "\n",
    "\n",
    "# sales x competition_distance - grouped by km\n",
    "competition_distance_km = list( np.arange(0, 20000, 1000) )\n",
    "aux1['competition_distance_km'] = pd.cut( aux1['competition_distance'], bins=competition_distance_km )\n",
    "aux2 = aux1[['competition_distance_km', 'sales']].groupby( 'competition_distance_km' ).sum().reset_index()\n",
    "sns.barplot( ax=axes[1], x='competition_distance_km', y='sales', data=aux2 );\n",
    "\n",
    "\n",
    "# sales x competition_distance corr method\n",
    "aux3 = aux1[['competition_distance', 'sales']]\n",
    "sns.heatmap( aux3.corr( method='pearson' ), annot=True, ax=axes[2] );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3 - Lojas com competidores a mais tempo deveriam vender mais (STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**FALSA** Lojas com COMPETIDORES A MAIS TEMPO vendem MENOS\n",
    "\n",
    "**COMPETITION_TIME_MONTH** possui um correlação com a variável resposta muito próxima de ZERO, portanto, BAIXA CORRELAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df3[['competition_time_month', 'sales']].groupby( 'competition_time_month' ).sum().reset_index()\n",
    "aux1 = aux1[ ( aux1['competition_time_month'] != 0 ) & ( aux1['competition_time_month'] <= 120 ) ]\n",
    "\n",
    "plt.figure(figsize=(30,10));\n",
    "plt.xticks( rotation='vertical');\n",
    "sns.barplot( x='competition_time_month', y='sales', data=aux1 );\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10) )\n",
    "fig.suptitle('Competition Time Month Behavior')\n",
    "\n",
    "# sales x competition_time_month \n",
    "sns.regplot( x='competition_time_month', y='sales', data=aux1, ax=axes[0] );\n",
    "\n",
    "# sales x competition_distance corr method\n",
    "sns.heatmap( aux1.corr( method='pearson' ), annot=True, ax=axes[1] );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4 - Lojas com promoções ativas a mais tempo deveriam vender mais (PRODUCT)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FALSA** Lojas com PROMOÇÔES ATIVAS A MAIS TEMPO vendem MENOS depois de um certo período de promoção\n",
    "\n",
    "**PROMO2_TIME_WEEK** possui uma correlação FRACA com a variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df3[['promo2_time_week', 'sales']].groupby( 'promo2_time_week').sum().reset_index()\n",
    "#sns.barplot( x='promo2_time_week', y='sales', data=aux1 );\n",
    "\n",
    "# extended promo\n",
    "aux2 = aux1[ aux1['promo2_time_week'] > 0 ] \n",
    "\n",
    "# regular promo\n",
    "aux3 = aux1[ aux1['promo2_time_week'] < 0 ] \n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(30, 10),  )\n",
    "fig.suptitle('Promo Time Week Behavior')\n",
    "\n",
    "axes[0].xaxis.set_tick_params( rotation=90 )\n",
    "axes[1].xaxis.set_tick_params( rotation=90 )\n",
    "\n",
    "\n",
    "sns.barplot( x='promo2_time_week', y='sales', data=aux2, ax=axes[0] );\n",
    "sns.barplot( x='promo2_time_week', y='sales', data=aux3, ax=axes[1] );\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5) )\n",
    "fig.suptitle('Promo Time Week Behavior')\n",
    "\n",
    "sns.regplot( x='promo2_time_week', y='sales', data=aux2, ax=axes[0] );\n",
    "sns.regplot( x='promo2_time_week', y='sales', data=aux3, ax=axes[1] );\n",
    "sns.heatmap( aux1.corr( method='pearson'), annot=True, ax=axes[2] );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <s> H5 - Lojas com mais dias de promoção deveriam vender mais (PRODUCT) </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ON HOLD** Será necessária a criação de uma variável com a quantidade de dias em promoção. Fica para a próxima rodada do CRISP-DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H6 - Lojas com mais promoções consecutivas deveriam vender mais (PRODUCT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FALSA** Lojas com MAIS PROMOÇÔES CONSECUTIVAS vendem MENOS\n",
    "\n",
    "**PROMO E PROMO2** aprensentam o mesmo comportamente em relação as vendas, portanto, ambas apresentam um correlação FRACA com a variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular and extended promo\n",
    "aux1 = df3[ ( df3['promo'] == 1 ) & ( df3['promo2'] == 1 ) ][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\n",
    "ax = aux1.plot()\n",
    "\n",
    "# regular promo\n",
    "aux2 = df3[ ( df3['promo'] == 1 ) & ( df3['promo2'] == 0 ) ][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\n",
    "aux2.plot( ax=ax )\n",
    "\n",
    "ax.legend( labels=['Regular + Extended Promo', 'Regular Promo'] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H7 -  Lojas abertas durante o feriado de Natal deveriam vender mais (TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FALSA**, Lojas abertas durante o feriado de Natal vendem MENOS\n",
    "\n",
    "**STATE_HOLIDAY** possui comportamentos distintos entre os tipos de feriados e por isso ter uma correlação FORTE com a variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df3[ df3['state_holiday'] != 'regular day'][['state_holiday', 'sales']].groupby( 'state_holiday' ).sum().reset_index()\n",
    "aux2 = df3[ df3['state_holiday'] != 'regular day'][['year', 'state_holiday', 'sales']].groupby( ['year', 'state_holiday'] ).sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5) )\n",
    "fig.suptitle('State Holiday Behavior')\n",
    "\n",
    "sns.barplot( x='state_holiday', y='sales', data=aux1, ax=axes[0] );\n",
    "sns.barplot( x='year', y='sales', hue='state_holiday', data=aux2, ax=axes[1] );\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H8 - Lojas deveriam vender mais ao longo dos anos (TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FALSA** Lojas vendem MENOS ao longo dos anos\n",
    "\n",
    "**YEAR** Possui uma correlação FORTE com a variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df3[['year', 'sales']].groupby( 'year' ).sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5) )\n",
    "fig.suptitle('Year Behavior')\n",
    "\n",
    "sns.barplot( x='year', y='sales', data=aux1, ax=axes[0] );\n",
    "sns.regplot( x='year', y='sales', data=aux1, ax=axes[1] );\n",
    "sns.heatmap( aux1.corr( method='pearson'), annot=True, ax=axes[2] );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H9 - Lojas deveriam vender mais no segundo semestre do ano (TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FALSA** Lojas vendem MENOS no segundo semestre do ANO\n",
    "\n",
    "**MONTH** possui uma ALTA correlação com a variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df3[['month', 'sales']].groupby( 'month' ).sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5) )\n",
    "fig.suptitle('Month Behavior')\n",
    "\n",
    "sns.barplot( x='month', y='sales', data=aux1, ax=axes[0] );\n",
    "sns.regplot( x='month', y='sales', data=aux1, ax=axes[1] );\n",
    "sns.heatmap( aux1.corr( method='pearson'), annot=True, ax=axes[2] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H10 - Lojas deveriam vender mais depois do dia 10 de cada mês (TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERDADEIRA** Lojas vendem MAIS depois do dia 10 de cada mês\n",
    "\n",
    "**DAY** possui uma correlação MÉDIA com a variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df3[['day', 'sales']].groupby( 'day' ).sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5) )\n",
    "fig.suptitle('Day Behavior')\n",
    "\n",
    "sns.barplot( x='day', y='sales', data=aux1, ax=axes[0] );\n",
    "sns.regplot( x='day', y='sales', data=aux1, ax=axes[1] );\n",
    "sns.heatmap( aux1.corr( method='pearson'), annot=True, ax=axes[2] );\n",
    "\n",
    "aux1['before_after'] = aux1['day'].apply( lambda x: 'before_day_10' if x <= 10 else 'after_day_10')\n",
    "aux2 = aux1[['before_after', 'sales']].groupby( 'before_after' ).sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20, 5) )\n",
    "fig.suptitle('Before/After Day 10 Behavior')\n",
    "\n",
    "sns.barplot( x='before_after', y='sales', data=aux2 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H11 -  Lojas deveriam vender menos nos finais de semana (TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERDADEIRA** Lojas vendem MENOS nos finais de semana\n",
    "\n",
    "**DAY_OF_WEEK** possui uma correlação ALTA com a variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df3[['day_of_week', 'sales']].groupby( 'day_of_week' ).sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5) )\n",
    "fig.suptitle('Day of Week Behavior')\n",
    "\n",
    "sns.barplot( x='day_of_week', y='sales', data=aux1, ax=axes[0] );\n",
    "sns.regplot( x='day_of_week', y='sales', data=aux1, ax=axes[1] );\n",
    "sns.heatmap( aux1.corr( method='pearson'), annot=True, ax=axes[2] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H12 - Lojas deveriam vender menos durante os feriados escolares (TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERDADEIRA** Lojas vendem MENOS durante  os feriados escolares, exceto mês de agosto\n",
    "\n",
    "**SCHOOL_HOLIDAY** possui uma correlação ALTA com a variável resposta, no entanto, possui pouca variação de comportamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df3[['school_holiday', 'sales']].groupby( 'school_holiday' ).sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5) )\n",
    "fig.suptitle('School Holiday Behavior')\n",
    "\n",
    "sns.barplot( x='school_holiday', y='sales', data=aux1, ax=axes[0] );\n",
    "sns.regplot( x='school_holiday', y='sales', data=aux1, ax=axes[1] );\n",
    "sns.heatmap( aux1.corr( method='pearson'), annot=True, ax=axes[2] );\n",
    "\n",
    "\n",
    "aux2 = df3[['month', 'school_holiday', 'sales']].groupby( ['month', 'school_holiday'] ).sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20, 5) )\n",
    "fig.suptitle('Month School Holiday Behavior')\n",
    "\n",
    "sns.barplot( x='month', hue='school_holiday', y='sales', data=aux2 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Resumo das Hipóteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = [ ['Hipóteses', 'Conclusão', 'Relevância'],\n",
    "        ['H1', 'Falsa', 'Baixa'],\n",
    "        ['H2', 'Falsa', 'Baixa'],\n",
    "        ['H3', 'Falsa', 'Baixa'],\n",
    "        ['H4', 'Falsa', 'Baixa'],\n",
    "        ['H5', '-', '-'],\n",
    "        ['H6', 'Falsa', 'Baixa'],\n",
    "        ['H7', 'Falsa', 'Alta'],\n",
    "        ['H8', 'Falsa', 'Alta'],\n",
    "        ['H9', 'Falsa', 'Alta'],\n",
    "        ['H10', 'Verdadeira', 'Média'],\n",
    "        ['H11', 'Verdadeira', 'Alta'],\n",
    "        ['H12', 'Verdadeira', 'Baixa'],\n",
    "]\n",
    "\n",
    "print( tabulate( tab, headers='firstrow' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df_num_attributes.corr( method='pearson' )\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(30, 10) )\n",
    "fig.suptitle('Multivariate Analysis - Numerical Attribues ')\n",
    "\n",
    "sns.heatmap( correlation, annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with new categorical columns\n",
    "cat = df3.select_dtypes( include='object' )\n",
    "cat.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cramer´s v for all columns combinations \n",
    "cv1 = cramer_v( cat['state_holiday'], cat['state_holiday'] )\n",
    "cv2 = cramer_v( cat['state_holiday'], cat['store_type'] )\n",
    "cv3 = cramer_v( cat['state_holiday'], cat['assortment'] )\n",
    "\n",
    "cv4 = cramer_v( cat['store_type'], cat['state_holiday'] )\n",
    "cv5 = cramer_v( cat['store_type'], cat['store_type'] )\n",
    "cv6 = cramer_v( cat['store_type'], cat['assortment'] )\n",
    "\n",
    "cv7 = cramer_v( cat['assortment'], cat['state_holiday'] )\n",
    "cv8 = cramer_v( cat['assortment'], cat['store_type'] )\n",
    "cv9 = cramer_v( cat['assortment'], cat['assortment'] )\n",
    "\n",
    "# create final dataframe\n",
    "df = pd.DataFrame( { 'state_holiday' : [cv1, cv2, cv3],\n",
    "                     'store_type'    : [cv4, cv5, cv6],\n",
    "                     'assortment'    : [cv7, cv8, cv9],\n",
    "                    })\n",
    "df = df.set_index( df.columns )\n",
    "\n",
    "# plot correlations between categorical attributes\n",
    "sns.heatmap( df, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Normalization Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMENTÁRIO:** Ao observar a distribuição das variáveis numéricas (seção 3.1.2), nota-se que NÃO HÁ variáveis com distribuição NORMAL, portanto, não há sentido em executar o método de normalização nesse conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Rescaling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df4[['date', 'sales', 'promo2_time_week', 'year', 'competition_distance', 'competition_time_month']]\n",
    "df4 = rescaling_features( df4, 'train' )\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 10))\n",
    "fig.suptitle('Rescaling Method - Before and After Scale')\n",
    "\n",
    "# promo2_time_wee - Few outliers - MinMax Scaler\n",
    "sns.boxplot( aux['promo2_time_week'], ax=axes[0,0] );\n",
    "sns.boxplot( df4['promo2_time_week'], ax=axes[0,1] );\n",
    "\n",
    "# year - Few outilies - MinMax Scaler\n",
    "sns.boxplot( aux['year'], ax=axes[1,0] );\n",
    "sns.boxplot( df4['year'], ax=axes[1,1] );\n",
    "\n",
    "# competition_distance - Many outliers - Robuster Scaler\n",
    "sns.boxplot( aux['competition_distance'], ax=axes[2,0] );\n",
    "sns.boxplot( df4['competition_distance'], ax=axes[2,1] );\n",
    "\n",
    "# competition_time_month - Many outliers - Robuster Scaler\n",
    "sns.boxplot( aux['competition_time_month'], ax=axes[3,0] );\n",
    "sns.boxplot( df4['competition_time_month'], ax=axes[3,1] );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Transformation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = encoding_features( df4, 'train' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Encoding Transformation for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state holiday column - One Hot Encoding\n",
    "df4[['state_holiday_public holiday', 'state_holiday_regular day', 'state_holiday_easter holiday', 'state_holiday_christmas']].sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store type column - Label encoding\n",
    "df4[['store', 'date', 'store_type']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assortment column - Ordinal encoding\n",
    "df4[['store', 'date', 'assortment']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Response Variable Magnitude Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df4[['date', 'sales']]\n",
    "\n",
    "df4 = magnitude_features( df4 )\n",
    "\n",
    "aux['new_sales'] = df4['sales']\n",
    "aux.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Nature Transformation - Variables with cyclical behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = ciclyc_features( df4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month\n",
    "df4[['month', 'month_sin', 'month_cos']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day_of_week\n",
    "df4[['day_of_week', 'day_of_week_sin', 'day_of_week_cos']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day\n",
    "df4[['day', 'day_sin', 'day_cos']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week_of_year\n",
    "df4[['week_of_year', 'week_of_year_sin', 'week_of_year_cos']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all features that created new features\n",
    "cols_drop = ['week_of_year', 'month', 'day', 'day_of_week', 'promo2_since', 'competition_since', 'year_week']\n",
    "df5 = df5.drop( cols_drop, axis=1 )\n",
    "\n",
    "# create training dataset \n",
    "X_train = df5.copy()\n",
    "y_train = X_train['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Boruta as Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training and test as array values to fit Boruta\n",
    "# X_train_array = X_train.drop( ['date', 'sales'], axis=1 ).values\n",
    "# y_train_array = y_train.values.ravel()\n",
    "\n",
    "# define Random Forest Regressor\n",
    "# rf = RandomForestRegressor( n_jobs=-1)\n",
    "\n",
    "# define Boruta\n",
    "# boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_array, y_train_array )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Selected and NOT Selected Features by Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cols selected and not selected by boruta\n",
    "# X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n",
    "\n",
    "# best features\n",
    "# cols_selected = boruta.support_.tolist()\n",
    "# cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.tolist()\n",
    "\n",
    "# not selected boruta\n",
    "# cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected by boruta\n",
    "cols_selected_boruta = ['store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month',\n",
    "                        'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', 'competition_time_month',\n",
    "                        'promo2_time_week', 'month_cos', 'day_of_week_sin', 'day_of_week_cos', 'day_sin', 'day_cos', 'week_of_year_cos']\n",
    "cols_selected_boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features NOT selected by boruta\n",
    "cols_not_selected_boruta = ['is_promo', 'month_sin', 'school_holiday', 'state_holiday_christmas', 'state_holiday_easter holiday',\n",
    "                            'state_holiday_public holiday', 'state_holiday_regular day', 'week_of_year_sin', 'year']\n",
    "cols_not_selected_boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Final Feature Selection by Me and Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with all features selected by me and Boruta, add date and sales columns\n",
    "#final_features = cols_selected_boruta.copy()\n",
    "#final_features.extend( ['date', 'sales', 'month_sin', 'week_of_year_sin'] )\n",
    "\n",
    "\n",
    "final_features = ['store', 'date', 'sales', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month', 'competition_open_since_year',\n",
    "                  'promo2', 'promo2_since_week', 'promo2_since_year', 'competition_time_month', 'promo2_time_week', \n",
    "                  'day_of_week_sin', 'day_of_week_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Create Train Dataset with Features to Apply ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets with final features and csv file\n",
    "X_train[final_features].to_csv( '../data/train_feat.csv', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Machine Learning Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv( '../data/train_feat.csv',  low_memory= False )\n",
    "X_train['date'] = pd.to_datetime( X_train['date'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Split df_feat_train in two datasets: X_train( full ) and X_validation( last 30 weeks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate period - last 36  weeks\n",
    "last_date  = X_train[['store', 'date']].groupby( 'store' ).max().reset_index().loc[0, 'date']\n",
    "first_date = last_date - datetime.timedelta( days=5*6*7 )\n",
    "\n",
    "last_date_period = last_date\n",
    "\n",
    "print(f'First date: {first_date}')\n",
    "print(f'Last date: {last_date}')\n",
    "\n",
    "# creating full datasets: X_train e X_validation\n",
    "X_validation = X_train[ X_train['date'] > first_date ]\n",
    "\n",
    "# create x_train, y_train variables\n",
    "x_train = X_train[ X_train['date'] <= first_date ]\n",
    "y_train = x_train['sales']\n",
    "x_train = x_train.drop( ['date', 'sales'], axis=1 )\n",
    "\n",
    "# calculate period - first 6 weeks from validation dataset\n",
    "first_date = X_validation[['store', 'date']].groupby( 'store' ).min().reset_index().loc[0, 'date']\n",
    "last_date  = first_date + datetime.timedelta( days=6*7 )\n",
    "\n",
    "print(f'\\nFirst date Validation: {first_date}')\n",
    "print(f'Last date Validation: {last_date}')\n",
    "\n",
    "# create x_validation, y_validation variables\n",
    "x_validation = X_validation[ X_validation['date'] < last_date ]\n",
    "y_validation = x_validation['sales']\n",
    "x_validation = x_validation.drop( ['date', 'sales'], axis=1 )\n",
    "\n",
    "print(f'\\n { last_date - first_date }' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aux_df with database test + response variable (sales)\n",
    "aux1          = x_validation.copy()\n",
    "aux1['sales'] = y_validation.copy()\n",
    "\n",
    "# prediction\n",
    "aux2 = aux1[['store', 'sales']].groupby( 'store' ).mean().reset_index().rename( columns = {'sales' :  'sales_predictions' } )\n",
    "aux1 = pd.merge( aux1, aux2, how='left', on='store' )\n",
    "yhat_avg = aux1['sales_predictions']\n",
    "\n",
    "# calculate performance with response variable \"sales\" on original scale \n",
    "avg_model_result = ml_error( 'Average Model', np.expm1( y_validation ), np.expm1( yhat_avg ) )\n",
    "avg_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Linear Regression - Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "lr_model = LinearRegression().fit( x_train, y_train )\n",
    "\n",
    "# create predictions\n",
    "yhat_lr = lr_model.predict( x_validation )\n",
    "\n",
    "# calculate performance\n",
    "lr_model_result = ml_error( 'Linear Regression Model', np.expm1( y_validation ), np.expm1( yhat_lr) )\n",
    "lr_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Linear Regression - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_result_cv = cross_validation( X_train, 5, 'Linear Regression', lr_model, verbose=True )\n",
    "lr_model_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Liner Regression Regularized (Lasso) - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 Lasso - Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "lasso_model = Lasso( alpha=0.01 ).fit( x_train, y_train )\n",
    "\n",
    "# create predictions\n",
    "yhat_lasso = lasso_model.predict( x_validation )\n",
    "\n",
    "# calculate performance\n",
    "lasso_model_result = ml_error( 'Linear Regression Regularized Model - Lasso', np.expm1( y_validation), np.expm1( yhat_lasso) )\n",
    "lasso_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 Lasso - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_result_cv = cross_validation( X_train, 5, 'Linear Regression Regularized - Lasso', lasso_model, verbose=True )\n",
    "lasso_model_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 Random Forest - Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# rf_model = RandomForestRegressor( n_estimators=100, n_jobs=-1, random_state=42 ).fit( x_train, y_train )\n",
    "\n",
    "# create predictions\n",
    "# yhat_rf = rf_model.predict( x_validation )\n",
    "\n",
    "# calculate performance\n",
    "# rf_model_result = ml_error( 'Random Forest Model', np.expm1( y_validation ), np.expm1( yhat_rf) )\n",
    "# rf_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 Random Forest - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_result_cv = cross_validation( X_train, 5, 'Random Forest Model', rf_model, verbose=True )\n",
    "# rf_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 XGBoost Regressor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.1 XGBoost - Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model with default values of XGBoost's parameter\n",
    "xgb_model = xgb.XGBRegressor( objective='reg:squarederror', \n",
    "                              n_estimators=100,\n",
    "                              eta=0.3, \n",
    "                              max_depth=6,  \n",
    "                              subsample=1,\n",
    "                              colsample_bytree=1,\n",
    "                              min_child_weight=1\n",
    "                            ).fit( x_train, y_train )\n",
    "\n",
    "# create predictions\n",
    "yhat_xgb = xgb_model.predict( x_validation )\n",
    "\n",
    "# calculate performance\n",
    "xgb_model_result = ml_error( 'XGBoost Regressor', np.expm1( y_validation ), np.expm1( yhat_xgb) )\n",
    "xgb_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.2 XGBoost - Cross Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_result_cv = cross_validation( X_train, 5, 'XGBoost Regressor', xgb_model, verbose=True )\n",
    "xgb_model_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Models Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.7 Single Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result = pd.concat( [avg_model_result, lr_model_result, lasso_model_result, xgb_model_result] )\n",
    "models_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.2 Cross Validation Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_result_cv = pd.concat( [lr_model_result_cv, lasso_model_result_cv, xgb_model_result_cv] )\n",
    "models_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Hyperparameter Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict with names and values parameters\n",
    "params = { 'n_estimators'     : [150, 170, 250, 300, 350],\n",
    "           'eta'              : [0.1, 0.3],\n",
    "           'max_depth'        : [3, 5, 9],\n",
    "           'subsample'        : [0.1, 0.5, 0.7],\n",
    "           'colsample_bytree' : [0.3, 0.5, 0.7],\n",
    "           'min_child_weight' : [3, 8, 15]\n",
    "          }\n",
    "\n",
    "NUM_ITERATIONS = 10\n",
    "xgb_model_result = pd.DataFrame()\n",
    "\n",
    "for i in range( NUM_ITERATIONS ):\n",
    "    # choose values for parameters randomly\n",
    "    hp = { k: random.sample( v, 1)[0] for k, v in params.items() }\n",
    "    print(f'Iteration n: { i+1 }. Parameters: { hp } ')\n",
    "\n",
    "    # define model\n",
    "    xgb_model = xgb.XGBRegressor( objective='reg:squarederror', \n",
    "                                  n_estimators=hp['n_estimators'],\n",
    "                                  eta=hp['eta'], \n",
    "                                  max_depth=hp['max_depth'], \n",
    "                                  subsample=hp['subsample'],\n",
    "                                  colsample_bytree=hp['colsample_bytree'],\n",
    "                                  min_child_weight=hp['min_child_weight'],\n",
    "                                ).fit( x_train, y_train )\n",
    "\n",
    "    # create predictions\n",
    "    yhat_xgb = xgb_model.predict( x_validation )\n",
    "\n",
    "    # calculate performance\n",
    "    result = cross_validation( X_train, 5, 'XGBoost Regressor', xgb_model, verbose=False )\n",
    "    xgb_model_result = pd.concat( [xgb_model_result, result] )\n",
    "\n",
    "xgb_model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Prepare train and test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "X_train         = pd.read_csv( '../data/train_feat.csv',  low_memory= False )\n",
    "X_train['date'] = pd.to_datetime( X_train['date'] )\n",
    "\n",
    "x_train = X_train.copy()\n",
    "x_train = x_train[final_features]\n",
    "y_train = x_train['sales']\n",
    "x_train = x_train.drop( columns=['date', 'sales'], axis=1 )\n",
    "\n",
    "x_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Execute Model with Test DataSet (new data) and Hyperparameters fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST CROSS VALIDATION RESULT\n",
    "\t#XGBoost Regressor\t910.97 +/- 155.77\t0.13 +/- 0.01\t1299.75 +/- 235.34\n",
    "  #Iteration n: 9. Parameters: {'n_estimators': 250, 'eta': 0.3, 'max_depth': 9, 'subsample': 0.5, 'colsample_bytree': 0.5, 'min_child_weight': 8} \n",
    "\n",
    "\n",
    "# define model\n",
    "xgb_model_tuned = xgb.XGBRegressor( objective='reg:squarederror', \n",
    "                                    n_estimators=250,\n",
    "                                    eta=0.3, \n",
    "                                    max_depth=9,  \n",
    "                                    subsample=0.5,\n",
    "                                    colsample_bytree=0.5,\n",
    "                                    min_child_weight=8\n",
    "                                  ).fit( x_train, y_train )\n",
    "\n",
    "xgb_model_tuned_result_cv = cross_validation( X_train, 5, 'XGBoost Regressor', xgb_model_tuned, verbose=True )\n",
    "xgb_model_tuned_result_cv\n",
    "\n",
    "# create predictions\n",
    "#yhat_xgb_tuned = xgb_model_tuned.predict( x_test )\n",
    "\n",
    "# calculate performance\n",
    "#xgb_model_tuned_result = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb_tuned) )\n",
    "#xgb_model_tuned_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Trained Model\n",
    "pickle.dump( xgb_model_tuned, open( '../model/model_rossmann.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Translating Model´s Performance to Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy full test dataset\n",
    "df_business = X_test[['Store', 'Date', 'Sales', 'Open']]\n",
    "df_business = rename_columns( df_business )\n",
    "df_business = df_business[ ~( ( df_business['sales'] == 0 ) | ( df_business['open'] == 0 ) ) ]\n",
    "\n",
    "df_business['sales_predictions']  = np.expm1( yhat_xgb_tuned )\n",
    "df_business['error_value']        = np.abs( df_business['sales'] - df_business['sales_predictions'] )\n",
    "df_business['error_percentage']   = np.round( np.abs( df_business['sales'] - df_business['sales_predictions'] ) / df_business['sales'], 4 )\n",
    "\n",
    "df_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business[df_business['store'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business[df_business['store'] == 2][['store', 'error_value']].groupby('store').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store´s performance\n",
    "df_store_performance = df_business[['store', 'sales', 'sales_predictions', 'error_value']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "# calculate MAE \n",
    "df_MAE = ( df_store_performance[['store', 'sales', 'sales_predictions']]\n",
    "                                .groupby( 'store' )\n",
    "                                .apply( lambda x: mean_absolute_error( x['sales'], x['sales_predictions'] ) )\n",
    "                                .reset_index()\n",
    "                                .rename( columns = { 0: 'MAE' } ) )\n",
    "\n",
    "df_MAPE = ( df_store_performance[['store', 'sales', 'sales_predictions']]\n",
    "            .groupby( 'store' )\n",
    "            .apply( lambda x: mean_absolute_percentage_error( x['sales'], x['sales_predictions'] ) )\n",
    "            .reset_index()\n",
    "            .rename( columns = { 0: 'MAPE' } ) )\n",
    "\n",
    "# merge dataframes SALES_PREDICTIONS AND MAE\n",
    "df_store_performance = pd.merge( df_store_performance, df_MAE, how='inner', on='store' )\n",
    "df_store_performance = pd.merge( df_store_performance, df_MAPE, how='inner', on='store' )\n",
    "\n",
    "# calculate values\n",
    "df_store_performance['worst_scenario'] = df_store_performance['sales_predictions'] - df_store_performance['error_value']\n",
    "df_store_performance['best_scenario']  = df_store_performance['sales_predictions'] + df_store_performance['error_value']\n",
    "#df_store_performance['MAPE']           = np.round( np.abs( df_store_performance['error_value']  / df_store_performance['sales_predictions'] ) * 100, 2 )\n",
    "\n",
    "df_store_performance['model_performance'] = df_store_performance['MAPE'].apply( lambda x : 'Excellent' if ( x <= 10.0 ) else \n",
    "                                                                                           'Good'      if ( x > 10.0 ) & ( x <= 15.0 ) else\n",
    "                                                                                           'Regular'   if ( x > 15.0 ) & ( x <= 20.0 ) else \n",
    "                                                                                           'Insufficient')\n",
    "\n",
    "\n",
    "df_store_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_performance.sort_values('MAPE', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melhores\n",
    "aux = X_train[ X_train['store'].isin( [782] ) ]\n",
    "aux['sales'] = np.expm1( X_train['sales'] )\n",
    "aux['year_month'] = aux['date'].dt.strftime('%Y-%m')\n",
    "aux['year'] = aux['date'].dt.strftime('%Y')\n",
    "aux['quarter'] = aux['date'].dt.quarter.astype(str)\n",
    "aux['year_quarter'] = aux['year'] + '-' + aux['quarter']\n",
    "\n",
    "aux = aux.sort_values('year_quarter')\n",
    "\n",
    "sns.lineplot( x='year_quarter', y='sales', data=aux, label='Sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_performance.sort_values('MAPE', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piores\n",
    "aux = X_train[ X_train['store'].isin( [198]) ]\n",
    "aux['sales'] = np.expm1( X_train['sales'] )\n",
    "aux['year_month'] = aux['date'].dt.strftime('%Y-%m')\n",
    "aux['quarter'] = aux['date'].dt.quarter.astype(str)\n",
    "aux['year'] = aux['date'].dt.strftime('%Y')\n",
    "aux['year_quarter'] = aux['year'] + '-' + aux['quarter']\n",
    "\n",
    "aux = aux.sort_values('year_quarter')\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot( x='year_quarter', y='sales', data=aux, label='Sales');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux[['store', 'year_quarter', 'sales']].groupby(['store', 'year_quarter']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = X_train.copy()\n",
    "\n",
    "aux['quarter'] = aux['date'].dt.quarter.astype(str)\n",
    "aux['year'] = aux['date'].dt.strftime('%Y')\n",
    "aux['year_quarter'] = aux['year'] + '-' + aux['quarter']\n",
    "\n",
    "aux1 = aux[['store', 'year_quarter']].groupby(['store']).nunique().reset_index()\n",
    "\n",
    "aux1 = aux1[ aux1['year_quarter'] < 10 ]\n",
    "aux1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux2 = df_store_performance = pd.merge( df_store_performance, aux1, how='left', on='store')\n",
    "aux2 = aux2[ ~aux2['year_quarter'].isna() ] \n",
    "aux2.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Calculate MPE Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_total_performance = ( df_store_performance[['sales_predictions', 'worst_scenario', 'best_scenario']]\n",
    "                            .apply( lambda x: np.sum(x), axis=0 )\n",
    "                            .reset_index()\n",
    "                            .rename( columns = {'index' : 'Scenario', 0 : 'Values' } ) \n",
    "                        )\n",
    "df3_total_performance['Values'] = df3_total_performance['Values'].map( 'R$ {:,.2f}'.format )\n",
    "df3_total_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe = mean_percentage_error( np.expm1( y_test ), np.expm1( yhat_xgb_tuned) )\n",
    "mpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smape_error( y, yhat ):\n",
    "    return round( np.mean( np.abs( yhat - y ) / ( ( np.abs( yhat)  + np.abs( y ) ) / 2 ) ) *100, 2) \n",
    "\n",
    "smape = smape_error( np.expm1( y_test ), np.expm1( yhat_xgb_tuned) )\n",
    "smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor( z ):\n",
    "    if z>0:\n",
    "        return \"blue\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "    \n",
    "fig, axes = plt.subplots(1, 1, figsize=(20, 10) )\n",
    "fig.suptitle('Sales vs Predictions')\n",
    "\n",
    "sns.lineplot( x='date', y='sales', data=df_business, label='Sales');\n",
    "sns.lineplot( x='date', y='sales_predictions', data=df_business, label='Predictions');\n",
    "\n",
    "\n",
    "#for x, y in zip( aux['date'], aux['sales'] ):\n",
    "#    plt.text(x = x,                       # x-coordinate position of data label\n",
    "#             y = y-150,                   # y-coordinate position of data label, adjusted to be 150 below the data point\n",
    "#             s = '{:,.2f}'.format(y),      # data label, formatted to ignore decimals\n",
    "#            color = 'green')             # set colour of line\n",
    "\n",
    "\n",
    "\n",
    "#for x, y, z in zip( aux['date'], aux['sales_predictions'], aux['error_percentage'] ):\n",
    "#    plt.text(x = x,                       # x-coordinate position of data label\n",
    "#             y = y-150,                   # y-coordinate position of data label, adjusted to be 150 below the data point\n",
    "#             s = '{:,.2f} ({:.2%})'.format( y, z ),      # data label, formatted to ignore decimals\n",
    "#            color = cor(z) )             # set colour of line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20, 10) )\n",
    "fig.suptitle('Sales vs Predictions')\n",
    "\n",
    "#plt.axhline( 10, linestyle='--')\n",
    "plt.axhline( 20, linestyle='--', color='red')\n",
    "sns.scatterplot( x='store', y='MAPE', data=df_store_performance);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20, 10) )\n",
    "fig.suptitle('Sales vs Predictions')\n",
    "\n",
    "aux = df_store_performance.sort_values('sales', ascending=False).head(50)\n",
    "\n",
    "sns.countplot( data=aux['model_performance'] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['store', 'date', 'sales', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month', 'competition_open_since_year',\n",
    "                  'promo2', 'promo2_since_week', 'promo2_since_year', 'competition_time_month', 'promo2_time_week', \n",
    "                  'day_of_week_sin', 'day_of_week_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos', 'month_sin', 'month_cos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 Call API Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40282, 17)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset with last 6 weeks - new data\n",
    "x_test = pd.read_csv( '../data/test.csv',  low_memory= False )\n",
    "\n",
    "# remove close day\n",
    "x_test = x_test[ ( x_test['open'] != 0 ) ]\n",
    "\n",
    "y_test = x_test['sales'].values\n",
    "x_test = x_test.drop( columns=['sales'], axis=1 )\n",
    "\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-07-31'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x_test['date'].max()\n",
    "#x1 = x1.dt.strftime('%Y-%m-%d')\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-30</td>\n",
       "      <td>8231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-29</td>\n",
       "      <td>7439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-28</td>\n",
       "      <td>8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-27</td>\n",
       "      <td>10047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-25</td>\n",
       "      <td>4725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-24</td>\n",
       "      <td>5187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9228</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-23</td>\n",
       "      <td>5434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-22</td>\n",
       "      <td>4946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11458</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-21</td>\n",
       "      <td>4985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12573</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-20</td>\n",
       "      <td>5519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14803</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-18</td>\n",
       "      <td>4925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15918</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>6474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17033</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-16</td>\n",
       "      <td>7744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18148</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-15</td>\n",
       "      <td>8132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19263</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-14</td>\n",
       "      <td>8516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20378</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-13</td>\n",
       "      <td>10041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22608</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-11</td>\n",
       "      <td>4869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23723</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-10</td>\n",
       "      <td>5812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24838</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-09</td>\n",
       "      <td>6469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25953</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-08</td>\n",
       "      <td>5925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27068</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>5396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28183</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-06</td>\n",
       "      <td>5771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30413</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-04</td>\n",
       "      <td>5085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31528</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>7899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32643</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>7972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33758</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>8726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34873</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>8735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35988</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>11186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38218</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-27</td>\n",
       "      <td>6079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39333</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-26</td>\n",
       "      <td>5171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40448</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-25</td>\n",
       "      <td>4771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41563</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>5763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42678</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>5539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43793</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-22</td>\n",
       "      <td>4754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46023</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-20</td>\n",
       "      <td>5001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       store        date  sales\n",
       "308      309  2015-07-31   8688\n",
       "1423     309  2015-07-30   8231\n",
       "2538     309  2015-07-29   7439\n",
       "3653     309  2015-07-28   8299\n",
       "4768     309  2015-07-27  10047\n",
       "6998     309  2015-07-25   4725\n",
       "8113     309  2015-07-24   5187\n",
       "9228     309  2015-07-23   5434\n",
       "10343    309  2015-07-22   4946\n",
       "11458    309  2015-07-21   4985\n",
       "12573    309  2015-07-20   5519\n",
       "14803    309  2015-07-18   4925\n",
       "15918    309  2015-07-17   6474\n",
       "17033    309  2015-07-16   7744\n",
       "18148    309  2015-07-15   8132\n",
       "19263    309  2015-07-14   8516\n",
       "20378    309  2015-07-13  10041\n",
       "22608    309  2015-07-11   4869\n",
       "23723    309  2015-07-10   5812\n",
       "24838    309  2015-07-09   6469\n",
       "25953    309  2015-07-08   5925\n",
       "27068    309  2015-07-07   5396\n",
       "28183    309  2015-07-06   5771\n",
       "30413    309  2015-07-04   5085\n",
       "31528    309  2015-07-03   7899\n",
       "32643    309  2015-07-02   7972\n",
       "33758    309  2015-07-01   8726\n",
       "34873    309  2015-06-30   8735\n",
       "35988    309  2015-06-29  11186\n",
       "38218    309  2015-06-27   6079\n",
       "39333    309  2015-06-26   5171\n",
       "40448    309  2015-06-25   4771\n",
       "41563    309  2015-06-24   5763\n",
       "42678    309  2015-06-23   5539\n",
       "43793    309  2015-06-22   4754\n",
       "46023    309  2015-06-20   5001"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_test[ (x_test['store'] == 309) ][['store', 'date', 'sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"store\": 309, \"day_of_week\": 5, \"date\": \"2015-07-31\", \"customers\": 707, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 4, \"date\": \"2015-07-30\", \"customers\": 641, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 3, \"date\": \"2015-07-29\", \"customers\": 574, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 2, \"date\": \"2015-07-28\", \"customers\": 615, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 1, \"date\": \"2015-07-27\", \"customers\": 709, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 6, \"date\": \"2015-07-25\", \"customers\": 451, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 5, \"date\": \"2015-07-24\", \"customers\": 496, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 4, \"date\": \"2015-07-23\", \"customers\": 504, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 3, \"date\": \"2015-07-22\", \"customers\": 456, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 2, \"date\": \"2015-07-21\", \"customers\": 441, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 1, \"date\": \"2015-07-20\", \"customers\": 514, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 6, \"date\": \"2015-07-18\", \"customers\": 442, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 5, \"date\": \"2015-07-17\", \"customers\": 518, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 4, \"date\": \"2015-07-16\", \"customers\": 631, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 3, \"date\": \"2015-07-15\", \"customers\": 588, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 2, \"date\": \"2015-07-14\", \"customers\": 596, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 1, \"date\": \"2015-07-13\", \"customers\": 694, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 6, \"date\": \"2015-07-11\", \"customers\": 448, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 5, \"date\": \"2015-07-10\", \"customers\": 526, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 4, \"date\": \"2015-07-09\", \"customers\": 588, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 3, \"date\": \"2015-07-08\", \"customers\": 505, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 2, \"date\": \"2015-07-07\", \"customers\": 458, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 1, \"date\": \"2015-07-06\", \"customers\": 536, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 6, \"date\": \"2015-07-04\", \"customers\": 470, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 5, \"date\": \"2015-07-03\", \"customers\": 601, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 4, \"date\": \"2015-07-02\", \"customers\": 604, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 3, \"date\": \"2015-07-01\", \"customers\": 619, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 2, \"date\": \"2015-06-30\", \"customers\": 631, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 1, \"date\": \"2015-06-29\", \"customers\": 772, \"open\": 1, \"promo\": 1, \"state_holiday\": 0, \"school_holiday\": 1, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 6, \"date\": \"2015-06-27\", \"customers\": 529, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 5, \"date\": \"2015-06-26\", \"customers\": 486, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 4, \"date\": \"2015-06-25\", \"customers\": 482, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 3, \"date\": \"2015-06-24\", \"customers\": 539, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 2, \"date\": \"2015-06-23\", \"customers\": 502, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 1, \"date\": \"2015-06-22\", \"customers\": 448, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}, {\"store\": 309, \"day_of_week\": 6, \"date\": \"2015-06-20\", \"customers\": 437, \"open\": 1, \"promo\": 0, \"state_holiday\": 0, \"school_holiday\": 0, \"store_type\": \"d\", \"assortment\": \"a\", \"competition_distance\": 8740.0, \"competition_open_since_month\": NaN, \"competition_open_since_year\": NaN, \"promo2\": 1, \"promo2_since_week\": 37.0, \"promo2_since_year\": 2009.0, \"promo_interval\": \"Feb,May,Aug,Nov\"}]'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# convert dataframe to json\n",
    "data = json.dumps( x_test.to_dict( orient='records' ) )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200 \n"
     ]
    }
   ],
   "source": [
    "# API CALL\n",
    "\n",
    "# local request\n",
    "url = 'http://0.0.0.0:5000/rossmann/predict'\n",
    "\n",
    "# Render´s Server request\n",
    "#url = 'https://rossmann-app-9l04.onrender.com/rossmann/predict'\n",
    "\n",
    "header = { 'Content-type' : 'application/json' }\n",
    "\n",
    "\n",
    "response = requests.post( url, data=data, headers=header )\n",
    "print(f'Status Code: { response.status_code } ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dataframe with predictions\n",
    "df_response = pd.DataFrame( response.json(), columns=response.json()[0].keys() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40282, 29)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response['sales'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1069.3499891493084"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( np.abs(d1['erro'].values) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38496.5996093751"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( np.abs(d1['erro'].values) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1658320546821019"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( np.abs( d1['sales'].values - d1['sales_predictions'].values ) / d1['sales'].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>sales_predictions</th>\n",
       "      <th>erro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-31T00:00:00.000</td>\n",
       "      <td>8688</td>\n",
       "      <td>9485.628906</td>\n",
       "      <td>-797.628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-30T00:00:00.000</td>\n",
       "      <td>8231</td>\n",
       "      <td>8284.787109</td>\n",
       "      <td>-53.787109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-29T00:00:00.000</td>\n",
       "      <td>7439</td>\n",
       "      <td>8479.685547</td>\n",
       "      <td>-1040.685547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3646</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-28T00:00:00.000</td>\n",
       "      <td>8299</td>\n",
       "      <td>8275.815430</td>\n",
       "      <td>23.184570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-27T00:00:00.000</td>\n",
       "      <td>10047</td>\n",
       "      <td>8708.862305</td>\n",
       "      <td>1338.137695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-25T00:00:00.000</td>\n",
       "      <td>4725</td>\n",
       "      <td>6151.291016</td>\n",
       "      <td>-1426.291016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-24T00:00:00.000</td>\n",
       "      <td>5187</td>\n",
       "      <td>6088.737305</td>\n",
       "      <td>-901.737305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8130</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-23T00:00:00.000</td>\n",
       "      <td>5434</td>\n",
       "      <td>5152.611328</td>\n",
       "      <td>281.388672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9243</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-22T00:00:00.000</td>\n",
       "      <td>4946</td>\n",
       "      <td>4927.962891</td>\n",
       "      <td>18.037109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10356</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-21T00:00:00.000</td>\n",
       "      <td>4985</td>\n",
       "      <td>5390.427734</td>\n",
       "      <td>-405.427734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11469</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-20T00:00:00.000</td>\n",
       "      <td>5519</td>\n",
       "      <td>5285.830566</td>\n",
       "      <td>233.169434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12614</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-18T00:00:00.000</td>\n",
       "      <td>4925</td>\n",
       "      <td>5527.189941</td>\n",
       "      <td>-602.189941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13727</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-17T00:00:00.000</td>\n",
       "      <td>6474</td>\n",
       "      <td>7249.235352</td>\n",
       "      <td>-775.235352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14839</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-16T00:00:00.000</td>\n",
       "      <td>7744</td>\n",
       "      <td>7048.789551</td>\n",
       "      <td>695.210449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15951</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-15T00:00:00.000</td>\n",
       "      <td>8132</td>\n",
       "      <td>6771.691895</td>\n",
       "      <td>1360.308105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17064</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-14T00:00:00.000</td>\n",
       "      <td>8516</td>\n",
       "      <td>7318.723633</td>\n",
       "      <td>1197.276367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18177</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-13T00:00:00.000</td>\n",
       "      <td>10041</td>\n",
       "      <td>8497.310547</td>\n",
       "      <td>1543.689453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19322</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-11T00:00:00.000</td>\n",
       "      <td>4869</td>\n",
       "      <td>6067.623047</td>\n",
       "      <td>-1198.623047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20436</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-10T00:00:00.000</td>\n",
       "      <td>5812</td>\n",
       "      <td>6338.745117</td>\n",
       "      <td>-526.745117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21550</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-09T00:00:00.000</td>\n",
       "      <td>6469</td>\n",
       "      <td>5163.790039</td>\n",
       "      <td>1305.209961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22664</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-08T00:00:00.000</td>\n",
       "      <td>5925</td>\n",
       "      <td>5068.407227</td>\n",
       "      <td>856.592773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23778</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-07T00:00:00.000</td>\n",
       "      <td>5396</td>\n",
       "      <td>5674.263672</td>\n",
       "      <td>-278.263672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24892</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-06T00:00:00.000</td>\n",
       "      <td>5771</td>\n",
       "      <td>5843.547852</td>\n",
       "      <td>-72.547852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26039</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-04T00:00:00.000</td>\n",
       "      <td>5085</td>\n",
       "      <td>6687.937988</td>\n",
       "      <td>-1602.937988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27153</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-03T00:00:00.000</td>\n",
       "      <td>7899</td>\n",
       "      <td>7627.588867</td>\n",
       "      <td>271.411133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28267</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-02T00:00:00.000</td>\n",
       "      <td>7972</td>\n",
       "      <td>7890.625977</td>\n",
       "      <td>81.374023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29381</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-01T00:00:00.000</td>\n",
       "      <td>8726</td>\n",
       "      <td>7357.161621</td>\n",
       "      <td>1368.838379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30496</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-30T00:00:00.000</td>\n",
       "      <td>8735</td>\n",
       "      <td>11126.500977</td>\n",
       "      <td>-2391.500977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31611</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-29T00:00:00.000</td>\n",
       "      <td>11186</td>\n",
       "      <td>9166.603516</td>\n",
       "      <td>2019.396484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32758</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-27T00:00:00.000</td>\n",
       "      <td>6079</td>\n",
       "      <td>5932.659668</td>\n",
       "      <td>146.340332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33873</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-26T00:00:00.000</td>\n",
       "      <td>5171</td>\n",
       "      <td>5697.426758</td>\n",
       "      <td>-526.426758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34987</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-25T00:00:00.000</td>\n",
       "      <td>4771</td>\n",
       "      <td>5069.601562</td>\n",
       "      <td>-298.601562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36101</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-24T00:00:00.000</td>\n",
       "      <td>5763</td>\n",
       "      <td>4896.900879</td>\n",
       "      <td>866.099121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37215</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-23T00:00:00.000</td>\n",
       "      <td>5539</td>\n",
       "      <td>5252.292969</td>\n",
       "      <td>286.707031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38329</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-22T00:00:00.000</td>\n",
       "      <td>4754</td>\n",
       "      <td>4902.378418</td>\n",
       "      <td>-148.378418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39475</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-20T00:00:00.000</td>\n",
       "      <td>5001</td>\n",
       "      <td>5824.311035</td>\n",
       "      <td>-823.311035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       store                     date  sales  sales_predictions         erro\n",
       "307      309  2015-07-31T00:00:00.000   8688        9485.628906  -797.628906\n",
       "1420     309  2015-07-30T00:00:00.000   8231        8284.787109   -53.787109\n",
       "2533     309  2015-07-29T00:00:00.000   7439        8479.685547 -1040.685547\n",
       "3646     309  2015-07-28T00:00:00.000   8299        8275.815430    23.184570\n",
       "4759     309  2015-07-27T00:00:00.000  10047        8708.862305  1338.137695\n",
       "5904     309  2015-07-25T00:00:00.000   4725        6151.291016 -1426.291016\n",
       "7017     309  2015-07-24T00:00:00.000   5187        6088.737305  -901.737305\n",
       "8130     309  2015-07-23T00:00:00.000   5434        5152.611328   281.388672\n",
       "9243     309  2015-07-22T00:00:00.000   4946        4927.962891    18.037109\n",
       "10356    309  2015-07-21T00:00:00.000   4985        5390.427734  -405.427734\n",
       "11469    309  2015-07-20T00:00:00.000   5519        5285.830566   233.169434\n",
       "12614    309  2015-07-18T00:00:00.000   4925        5527.189941  -602.189941\n",
       "13727    309  2015-07-17T00:00:00.000   6474        7249.235352  -775.235352\n",
       "14839    309  2015-07-16T00:00:00.000   7744        7048.789551   695.210449\n",
       "15951    309  2015-07-15T00:00:00.000   8132        6771.691895  1360.308105\n",
       "17064    309  2015-07-14T00:00:00.000   8516        7318.723633  1197.276367\n",
       "18177    309  2015-07-13T00:00:00.000  10041        8497.310547  1543.689453\n",
       "19322    309  2015-07-11T00:00:00.000   4869        6067.623047 -1198.623047\n",
       "20436    309  2015-07-10T00:00:00.000   5812        6338.745117  -526.745117\n",
       "21550    309  2015-07-09T00:00:00.000   6469        5163.790039  1305.209961\n",
       "22664    309  2015-07-08T00:00:00.000   5925        5068.407227   856.592773\n",
       "23778    309  2015-07-07T00:00:00.000   5396        5674.263672  -278.263672\n",
       "24892    309  2015-07-06T00:00:00.000   5771        5843.547852   -72.547852\n",
       "26039    309  2015-07-04T00:00:00.000   5085        6687.937988 -1602.937988\n",
       "27153    309  2015-07-03T00:00:00.000   7899        7627.588867   271.411133\n",
       "28267    309  2015-07-02T00:00:00.000   7972        7890.625977    81.374023\n",
       "29381    309  2015-07-01T00:00:00.000   8726        7357.161621  1368.838379\n",
       "30496    309  2015-06-30T00:00:00.000   8735       11126.500977 -2391.500977\n",
       "31611    309  2015-06-29T00:00:00.000  11186        9166.603516  2019.396484\n",
       "32758    309  2015-06-27T00:00:00.000   6079        5932.659668   146.340332\n",
       "33873    309  2015-06-26T00:00:00.000   5171        5697.426758  -526.426758\n",
       "34987    309  2015-06-25T00:00:00.000   4771        5069.601562  -298.601562\n",
       "36101    309  2015-06-24T00:00:00.000   5763        4896.900879   866.099121\n",
       "37215    309  2015-06-23T00:00:00.000   5539        5252.292969   286.707031\n",
       "38329    309  2015-06-22T00:00:00.000   4754        4902.378418  -148.378418\n",
       "39475    309  2015-06-20T00:00:00.000   5001        5824.311035  -823.311035"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = df_response[ (df_response['store'] == 309) ][['store', 'date', 'sales', 'sales_predictions']]\n",
    "d['erro'] = d['sales'] - d['sales_predictions']\n",
    "d.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>sales_predictions</th>\n",
       "      <th>erro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-31T00:00:00.000</td>\n",
       "      <td>8688</td>\n",
       "      <td>8986.851562</td>\n",
       "      <td>-298.851562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-30T00:00:00.000</td>\n",
       "      <td>8231</td>\n",
       "      <td>6760.920410</td>\n",
       "      <td>1470.079590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-29T00:00:00.000</td>\n",
       "      <td>7439</td>\n",
       "      <td>6529.138672</td>\n",
       "      <td>909.861328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-28T00:00:00.000</td>\n",
       "      <td>8299</td>\n",
       "      <td>6912.889160</td>\n",
       "      <td>1386.110840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-27T00:00:00.000</td>\n",
       "      <td>10047</td>\n",
       "      <td>8270.425781</td>\n",
       "      <td>1776.574219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-25T00:00:00.000</td>\n",
       "      <td>4725</td>\n",
       "      <td>3124.349121</td>\n",
       "      <td>1600.650879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-24T00:00:00.000</td>\n",
       "      <td>5187</td>\n",
       "      <td>4860.462891</td>\n",
       "      <td>326.537109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-23T00:00:00.000</td>\n",
       "      <td>5434</td>\n",
       "      <td>4811.437988</td>\n",
       "      <td>622.562012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-22T00:00:00.000</td>\n",
       "      <td>4946</td>\n",
       "      <td>4758.552246</td>\n",
       "      <td>187.447754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-21T00:00:00.000</td>\n",
       "      <td>4985</td>\n",
       "      <td>4908.414551</td>\n",
       "      <td>76.585449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-20T00:00:00.000</td>\n",
       "      <td>5519</td>\n",
       "      <td>4726.911621</td>\n",
       "      <td>792.088379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-18T00:00:00.000</td>\n",
       "      <td>4925</td>\n",
       "      <td>3389.483887</td>\n",
       "      <td>1535.516113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-17T00:00:00.000</td>\n",
       "      <td>6474</td>\n",
       "      <td>6237.249023</td>\n",
       "      <td>236.750977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-16T00:00:00.000</td>\n",
       "      <td>7744</td>\n",
       "      <td>6561.453125</td>\n",
       "      <td>1182.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-15T00:00:00.000</td>\n",
       "      <td>8132</td>\n",
       "      <td>6697.340820</td>\n",
       "      <td>1434.659180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-14T00:00:00.000</td>\n",
       "      <td>8516</td>\n",
       "      <td>7126.185547</td>\n",
       "      <td>1389.814453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-13T00:00:00.000</td>\n",
       "      <td>10041</td>\n",
       "      <td>8777.877930</td>\n",
       "      <td>1263.122070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-11T00:00:00.000</td>\n",
       "      <td>4869</td>\n",
       "      <td>3498.794434</td>\n",
       "      <td>1370.205566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-10T00:00:00.000</td>\n",
       "      <td>5812</td>\n",
       "      <td>5438.655273</td>\n",
       "      <td>373.344727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-09T00:00:00.000</td>\n",
       "      <td>6469</td>\n",
       "      <td>5089.354980</td>\n",
       "      <td>1379.645020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-08T00:00:00.000</td>\n",
       "      <td>5925</td>\n",
       "      <td>5063.135254</td>\n",
       "      <td>861.864746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-07T00:00:00.000</td>\n",
       "      <td>5396</td>\n",
       "      <td>5381.601562</td>\n",
       "      <td>14.398438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-06T00:00:00.000</td>\n",
       "      <td>5771</td>\n",
       "      <td>5564.568359</td>\n",
       "      <td>206.431641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-04T00:00:00.000</td>\n",
       "      <td>5085</td>\n",
       "      <td>3072.291016</td>\n",
       "      <td>2012.708984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-03T00:00:00.000</td>\n",
       "      <td>7899</td>\n",
       "      <td>6602.504883</td>\n",
       "      <td>1296.495117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-02T00:00:00.000</td>\n",
       "      <td>7972</td>\n",
       "      <td>7458.822266</td>\n",
       "      <td>513.177734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-07-01T00:00:00.000</td>\n",
       "      <td>8726</td>\n",
       "      <td>7425.523926</td>\n",
       "      <td>1300.476074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-30T00:00:00.000</td>\n",
       "      <td>8735</td>\n",
       "      <td>8390.232422</td>\n",
       "      <td>344.767578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-29T00:00:00.000</td>\n",
       "      <td>11186</td>\n",
       "      <td>9036.488281</td>\n",
       "      <td>2149.511719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-27T00:00:00.000</td>\n",
       "      <td>6079</td>\n",
       "      <td>2751.213379</td>\n",
       "      <td>3327.786621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-26T00:00:00.000</td>\n",
       "      <td>5171</td>\n",
       "      <td>4249.750000</td>\n",
       "      <td>921.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-25T00:00:00.000</td>\n",
       "      <td>4771</td>\n",
       "      <td>4247.132324</td>\n",
       "      <td>523.867676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-24T00:00:00.000</td>\n",
       "      <td>5763</td>\n",
       "      <td>4115.154785</td>\n",
       "      <td>1647.845215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-23T00:00:00.000</td>\n",
       "      <td>5539</td>\n",
       "      <td>4737.148926</td>\n",
       "      <td>801.851074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-22T00:00:00.000</td>\n",
       "      <td>4754</td>\n",
       "      <td>4249.296387</td>\n",
       "      <td>504.703613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>309</td>\n",
       "      <td>2015-06-20T00:00:00.000</td>\n",
       "      <td>5001</td>\n",
       "      <td>2544.490723</td>\n",
       "      <td>2456.509277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    store                     date  sales  sales_predictions         erro\n",
       "0     309  2015-07-31T00:00:00.000   8688        8986.851562  -298.851562\n",
       "1     309  2015-07-30T00:00:00.000   8231        6760.920410  1470.079590\n",
       "2     309  2015-07-29T00:00:00.000   7439        6529.138672   909.861328\n",
       "3     309  2015-07-28T00:00:00.000   8299        6912.889160  1386.110840\n",
       "4     309  2015-07-27T00:00:00.000  10047        8270.425781  1776.574219\n",
       "5     309  2015-07-25T00:00:00.000   4725        3124.349121  1600.650879\n",
       "6     309  2015-07-24T00:00:00.000   5187        4860.462891   326.537109\n",
       "7     309  2015-07-23T00:00:00.000   5434        4811.437988   622.562012\n",
       "8     309  2015-07-22T00:00:00.000   4946        4758.552246   187.447754\n",
       "9     309  2015-07-21T00:00:00.000   4985        4908.414551    76.585449\n",
       "10    309  2015-07-20T00:00:00.000   5519        4726.911621   792.088379\n",
       "11    309  2015-07-18T00:00:00.000   4925        3389.483887  1535.516113\n",
       "12    309  2015-07-17T00:00:00.000   6474        6237.249023   236.750977\n",
       "13    309  2015-07-16T00:00:00.000   7744        6561.453125  1182.546875\n",
       "14    309  2015-07-15T00:00:00.000   8132        6697.340820  1434.659180\n",
       "15    309  2015-07-14T00:00:00.000   8516        7126.185547  1389.814453\n",
       "16    309  2015-07-13T00:00:00.000  10041        8777.877930  1263.122070\n",
       "17    309  2015-07-11T00:00:00.000   4869        3498.794434  1370.205566\n",
       "18    309  2015-07-10T00:00:00.000   5812        5438.655273   373.344727\n",
       "19    309  2015-07-09T00:00:00.000   6469        5089.354980  1379.645020\n",
       "20    309  2015-07-08T00:00:00.000   5925        5063.135254   861.864746\n",
       "21    309  2015-07-07T00:00:00.000   5396        5381.601562    14.398438\n",
       "22    309  2015-07-06T00:00:00.000   5771        5564.568359   206.431641\n",
       "23    309  2015-07-04T00:00:00.000   5085        3072.291016  2012.708984\n",
       "24    309  2015-07-03T00:00:00.000   7899        6602.504883  1296.495117\n",
       "25    309  2015-07-02T00:00:00.000   7972        7458.822266   513.177734\n",
       "26    309  2015-07-01T00:00:00.000   8726        7425.523926  1300.476074\n",
       "27    309  2015-06-30T00:00:00.000   8735        8390.232422   344.767578\n",
       "28    309  2015-06-29T00:00:00.000  11186        9036.488281  2149.511719\n",
       "29    309  2015-06-27T00:00:00.000   6079        2751.213379  3327.786621\n",
       "30    309  2015-06-26T00:00:00.000   5171        4249.750000   921.250000\n",
       "31    309  2015-06-25T00:00:00.000   4771        4247.132324   523.867676\n",
       "32    309  2015-06-24T00:00:00.000   5763        4115.154785  1647.845215\n",
       "33    309  2015-06-23T00:00:00.000   5539        4737.148926   801.851074\n",
       "34    309  2015-06-22T00:00:00.000   4754        4249.296387   504.703613\n",
       "35    309  2015-06-20T00:00:00.000   5001        2544.490723  2456.509277"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = df_response[ (df_response['store'] == 309) ][['store', 'date', 'sales', 'sales_predictions']]\n",
    "d1['erro'] = d1['sales'] - d1['sales_predictions']\n",
    "d1.head(40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
